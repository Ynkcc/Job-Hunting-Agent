import streamlit as st
from playwright.sync_api import sync_playwright
from agents.JobAgent import GPTRanker, GPTFilter, ResumeLoader
from agents.SearchAgent import GetJobQueryStructure
from JobRender import render
from uiautomation import WindowControl
from loguru import logger
from crawl import get_job_info, login
import multiprocessing
import time
import json
import os

st.set_page_config(page_title="èŒä½æ™ºèƒ½æ¨è", page_icon="ğŸ–¥ï¸", layout="wide")

def crawling_thread(job_queue, url):
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=False)
        context = login(browser)
        job_set = set()
        page = context.new_page()
        page.goto(url)
        page2 = context.new_page()
    
        try:
            page.wait_for_selector(".options-pages a", timeout=10000)
        except:
            logger.warning("Timeout!")
            return 0

        num = min(int(page.locator(".options-pages a").all_inner_texts()[-2]), 1)
        for i in range(1, num+1):
            try:
                page.wait_for_selector(".job-list-box .job-card-wrapper", timeout=10000)
            except:
                logger.warning("ç­‰å¾…åŠ è½½å·¥ä½œç•Œé¢è¶…æ—¶ï¼")
                try:
                    page.locator(".ui-icon-arrow-right").click() 
                    continue
                except:
                    continue
            job_lists = page.locator(".job-list-box .job-card-wrapper").all()       # è·å–å½“å‰é¡µæ‰€æœ‰èŒä½
            for job in job_lists:
                jobinfo = get_job_info(job, allow_duplicate=True)
                if jobinfo is None or jobinfo.sent==1:
                    continue
                if hash(jobinfo) in job_set:
                    continue
                job_set.add(hash(jobinfo))
                try:
                    page2.goto(jobinfo.url)
                    page2.wait_for_selector(".job-sec-text", timeout=10000)
                except:
                    logger.warning(f"Failed to load {jobinfo.jobname}")
                    continue
                description = page2.locator(".job-sec-text").all_inner_texts()[0]
                jobinfo.description = description
                job_queue.put(jobinfo)
                try:
                    jobinfo.commit_to_db()
                except Exception as e:
                    logger.error(f"Failed to commit to db: {e}")
                time.sleep(0.5)
                
            job_queue.put((i, num))
            page.locator(".ui-icon-arrow-right").click()        # ç‚¹å‡»ä¸‹ä¸€é¡µ
            time.sleep(0.5)
        job_queue.put(None)      # æœ€åä¸€ä¸ªå…ƒç´ ä¸ºæœ€å¤§é¡µæ•°
        context.close()
        browser.close()

def sentCV(url, cv_path, message):
    loader = ResumeLoader(cv_path)
    png_path = loader.picture_path[0]
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=False)
        context = login(browser)
        page = context.new_page()
        page.goto(url)
        page.wait_for_timeout(2000)
        page.locator(".btn-container .btn-startchat").click()       # ç«‹å³æ²Ÿé€š
        page.wait_for_selector("#chat-input")
        input_box = page.locator("#chat-input")
        input_box.fill(message)
        page.wait_for_timeout(2000)
        page.locator("button[type=send]").click()
        page.wait_for_timeout(2000)
        page.locator(".toolbar-btn-content [type=file]").click()            # ç‚¹å‡»å‘é€ç®€å†ç…§ç‰‡
        page.wait_for_timeout(1000)
        openWindow = WindowControl(name='æ‰“å¼€')
        openWindow.SwitchToThisWindow()
        openWindow.EditControl(Name='æ–‡ä»¶å(N):').SendKeys(png_path)
        openWindow.ButtonControl(Name='æ‰“å¼€(O)').Click()
        time.sleep(1)

if __name__ == '__main__':
    uploaded_file = st.file_uploader("é€‰æ‹©ä½ çš„ç®€å†æ–‡ä»¶", type=["pdf"])

    TotalJobInfo = []
    col1, col2, col3, col4, col5 = st.columns(5)
    
    default_model = "gpt-4o-mini"
    default_batch_size = 10
    default_window_length = 20
    default_step = 10
    default_ratio = 0.5
    default_search_input = ""
    default_message_input = ""
    
    if os.path.exists("cache/last_search.json"):
        with open("cache/last_search.json", "r", encoding="utf-8") as f:
            js = json.load(f)
            default_model = js.get("model", default_model)
            default_batch_size = js.get("batch_size", default_batch_size)
            default_window_length = js.get("window_length", default_window_length)
            default_step = js.get("step", default_step)
            default_ratio = js.get("ratio", default_ratio)
            default_search_input = js.get("search_input", default_search_input)
            default_message_input = js.get("message_input", default_message_input)
            
    search_input = st.text_input("è¯·è¾“å…¥ä½ æƒ³æ‰¾çš„èŒä½", value=default_search_input)
    message_input = st.text_input("è¯·è¾“å…¥å‘é€ç»™BOSSçš„è¯æœ¯", value=default_message_input)
    with col1:
        model = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"], index=0)
    with col2:
        batch_size = st.number_input("è¯·è¾“å…¥æ‰¹é‡å¤„ç†çš„æ•°é‡", value=10, min_value=1, max_value=100)
    with col3:
        window_length = st.number_input("è¯·è¾“å…¥çª—å£é•¿åº¦", value=20, min_value=1, max_value=100)
    with col4:
        step = st.number_input("è¯·è¾“å…¥æ­¥é•¿", value=10, min_value=1, max_value=100)
    with col5:
        ratio = st.number_input("è¯·è¾“å…¥ç­›é€‰æ¯”ä¾‹", value=0.5, min_value=0.1, max_value=1.0)
        
    with open("cache/last_search.json", "w", encoding="utf-8") as f:
        json.dump({"model": model, "batch_size": batch_size, "window_length": window_length, "step": step, \
            "ratio": ratio, "search_input": search_input, "message_input": message_input}, f, ensure_ascii=False, indent=4)

    if st.button("å¼€å§‹æœç´¢"):
        if uploaded_file is None:
            st.error("è¯·é€‰æ‹©ç®€å†æ–‡ä»¶")
            st.stop()
        if search_input == "":
            st.error("è¯·è¾“å…¥å²—ä½è¦æ±‚")
            st.stop()
        jobquery = GetJobQueryStructure(search_input)
        url = jobquery.to_url()
        job_buffer = multiprocessing.Queue()
        writer_process = multiprocessing.Process(target=crawling_thread, args=(job_buffer, url))
        writer_process.start()
        
        bar = st.progress(0)
        while True:
            try:
                item = job_buffer.get()
                if item is None:
                    break
                if isinstance(item, tuple):
                    bar.progress(item[0]/item[1], f"å·²å®Œæˆ{item[0]}é¡µï¼Œå…±{item[1]}é¡µ")
                else:
                    TotalJobInfo.append(item)
            except KeyboardInterrupt:
                writer_process.terminate()
                break
            except Exception as e:
                logger.error(e)
                pass
                
        cv_path = uploaded_file.name
        st.write(f"å…±æ‰¾åˆ°{len(TotalJobInfo)}æ¡èŒä½ä¿¡æ¯")
        st.write("æ­£åœ¨æ ¹æ®ç”¨æˆ·éœ€æ±‚è¿›è¡Œè¿‡æ»¤...")
        filter = GPTFilter(TotalJobInfo, search_input)
        TotalJobInfo = filter.filter(batch_size, model=model)
        st.write(f"è¿‡æ»¤åå‰©ä½™{len(TotalJobInfo)}æ¡èŒä½ä¿¡æ¯\næ­£åœ¨æ’åºä¸­...")
        ranker = GPTRanker(TotalJobInfo, cv_path)  
        TotalJobInfo = ranker.rank(window_length=window_length, step=step, model=model)
        TotalJobInfo = TotalJobInfo[:int(len(TotalJobInfo)*ratio)]
        st.write(f"æ’åºåå‰©ä½™{len(TotalJobInfo)}æ¡èŒä½ä¿¡æ¯")
        
    render(TotalJobInfo)
    for job in TotalJobInfo:
        url = job.url
        process = multiprocessing.Process(target=sentCV, args=(url, cv_path, message_input))
        process.start()
        process.join()