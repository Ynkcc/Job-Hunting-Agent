from logging import log
import streamlit as st
from crawl import get_job_info, login
from playwright.sync_api import Playwright, sync_playwright
from APIDataClass import JobInfo, JobQueryRequest, cursor, connection
import streamlit.components.v1 as components
from loguru import logger
from JobRender import render
import pandas as pd
import json
import multiprocessing
import pyautogui
import os
import time

st.set_page_config(page_title="æ•°æ®æ ‡æ³¨å¹³å°", page_icon="ğŸ·ï¸", layout="wide")

@st.cache_data
def get_data(query):
    cursor.execute(query)
    data = cursor.fetchall()
    df = pd.DataFrame(data, columns=[desc[0] for desc in cursor.description])
    return df

def crawling_thread(job_queue, jobType, url, output_file):
    with sync_playwright() as playwright, open(output_file, 'a', encoding='utf-8') as f:
        browser = playwright.chromium.launch(headless=False)
        context = login(browser)
        job_set = set()
        page = context.new_page()
        page.goto(url)
        page2 = context.new_page()
    
        try:
            page.wait_for_selector(".options-pages a", timeout=10000)
        except:
            logger.warning("Timeout!")
            return 0
        
        # window_title = browser.contexts[0].pages[0].title()
        # window = pyautogui.getWindowsWithTitle(window_title)[0]
        # window.minimize()
        
        num = min(int(page.locator(".options-pages a").all_inner_texts()[-2]), 10)
        for i in range(1, num+1):
            try:
                page.wait_for_selector(".job-list-box .job-card-wrapper", timeout=10000)
            except:
                logger.warning("ç­‰å¾…åŠ è½½å·¥ä½œç•Œé¢è¶…æ—¶ï¼")
                try:
                    page.locator(".ui-icon-arrow-right").click() 
                    continue
                except:
                    continue
            job_lists = page.locator(".job-list-box .job-card-wrapper").all()       # è·å–å½“å‰é¡µæ‰€æœ‰èŒä½
            for job in job_lists:
                jobinfo = get_job_info(job, jobType)
                if jobinfo is None:
                    continue
                if hash(jobinfo) in job_set:
                    continue
                job_set.add(hash(jobinfo))
                try:
                    page2.goto(jobinfo.url)
                    page2.wait_for_selector(".job-sec-text", timeout=10000)
                except:
                    logger.warning(f"Failed to load {jobinfo.jobname}")
                    continue
                description = page2.locator(".job-sec-text").all_inner_texts()[0]
                try:
                    page2.wait_for_selector(".look-job-box .look-job-list a", timeout=10000)
                except:
                    continue
                other_related_jobs = page2.locator(".look-job-box .look-job-list a").all()
                urls = []
                for related_job in other_related_jobs:
                    related_job_url = f'https://www.zhipin.com{related_job.get_attribute("href")}'
                    urls.append(related_job_url)
                jobinfo.description = description
                job_queue.put(jobinfo)
                try:
                    jobinfo.commit_to_db()
                except Exception as e:
                    logger.error(f"Failed to commit to db: {e}")
                data = {"jobname": jobinfo.jobname, "company": jobinfo.company, "city": jobinfo.city, "url": jobinfo.url, "related_jobs": urls}
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
                time.sleep(0.5)
                
            job_queue.put((i, num))
            page.locator(".ui-icon-arrow-right").click()        # ç‚¹å‡»ä¸‹ä¸€é¡µ
            time.sleep(0.5)
        job_queue.put(None)      # æœ€åä¸€ä¸ªå…ƒç´ ä¸ºæœ€å¤§é¡µæ•°
        context.close()
        browser.close()
        
def main():
    jobtypes = get_data("SELECT DISTINCT name FROM jobtype;")['name'].tolist()
    city_json = json.load(open('metadata/city.json', 'r', encoding='utf-8'))
    provinces = []
    for item in city_json["zpData"]["cityList"]:
        provinces.append(item["name"])
    industries = get_data("SELECT DISTINCT name FROM industry;")['name'].tolist()
    degrees = list(JobQueryRequest.degree_map.keys())
    experiences = list(JobQueryRequest.experience_map.keys())
    scales = list(JobQueryRequest.scale_map.keys())
    stages = list(JobQueryRequest.stage_map.keys())

    selected_province = st.sidebar.selectbox(
        "è¯·é€‰æ‹©çœä»½",
        options=['ä¸é™'] + provinces,
        index=1
    )
    
    if selected_province == 'ä¸é™':
        cities = []
    else:
        for item in city_json["zpData"]["cityList"]:
            if item["name"] == selected_province:
                cities = [city["name"] for city in item["subLevelModelList"]]
                break

    selected_city = st.sidebar.selectbox(
        "è¯·é€‰æ‹©åŸå¸‚",
        options=['ä¸é™'] + cities,
        index=1 if cities else 0
    )

    if selected_city == 'ä¸é™':
        regions = []
    else:
        for item in city_json["zpData"]["cityList"]:
            if item["name"] == selected_province:
                for subitem in item["subLevelModelList"]:
                    if subitem["name"] == selected_city:
                        regions = [region["name"] for region in subitem["subLevelModelList"]]
                        break
                break

    selected_region = st.sidebar.selectbox("è¯·é€‰æ‹©åŒºåŸŸ", options=['ä¸é™'] + regions, index=0)
    selected_jobtype = st.sidebar.multiselect("è¯·é€‰æ‹©èŒä½ç±»å‹", options=['ä¸é™'] + jobtypes, default=['ä¸é™'])
    selected_industry = st.sidebar.multiselect("è¯·é€‰æ‹©è¡Œä¸š", options=['ä¸é™'] + industries, default=['äº’è”ç½‘'])
    selected_degree = st.sidebar.multiselect("è¯·é€‰æ‹©å­¦å†", options=['ä¸é™'] + degrees, default=['ä¸é™'])
    selected_experience = st.sidebar.multiselect("è¯·é€‰æ‹©ç»éªŒè¦æ±‚", options=['ä¸é™'] + experiences, default=['ä¸é™'])
    selected_scale = st.sidebar.multiselect("è¯·é€‰æ‹©äººå‘˜è§„æ¨¡", options=['ä¸é™'] + scales, default=['ä¸é™'])
    selected_stage = st.sidebar.multiselect("è¯·é€‰æ‹©å…¬å¸èèµ„é˜¶æ®µ", options=['ä¸é™'] + stages, default=['ä¸é™'])

    # åœ¨ç¬¬ä¸€ä¸ªåˆ—ä¸­æ·»åŠ æ–‡æœ¬è¾“å…¥æ¡†
    keyword = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯", placeholder='ä¾‹å¦‚ï¼šæ•°æ®åˆ†æ')
    
    TotalJobInfo = []
    
    if not os.path.exists('cache/dataset/'):
        os.makedirs('cache/dataset/')
    output_dirs = os.listdir('cache/dataset/')
    selected_file_or_new = st.selectbox(
        "åŠ è½½å·²æœ‰æ•°æ®é›†æˆ–è¾“å…¥æ–°æ•°æ®é›†åç§°ï¼š",
        options=["<è¾“å…¥æ–°æ•°æ®é›†åç§°>"] + output_dirs
    )
    if selected_file_or_new == "<è¾“å…¥æ–°æ•°æ®é›†åç§°>":
        new_dataset_name = st.text_input("è¯·è¾“å…¥æƒ³è¦ä¿å­˜çš„æ•°æ®é›†åç§°ï¼š", placeholder='ä¾‹å¦‚ï¼šoriginal')
        if new_dataset_name:
            if new_dataset_name in output_dirs:
                st.warning("æ•°æ®é›†å·²å­˜åœ¨ï¼")
            output_file = f'cache/dataset/{new_dataset_name}/raw_data.jsonl'
            if st.button("æäº¤"):
                bar = st.progress(0, "æ­£åœ¨è·å–èŒä½ä¿¡æ¯ï¼Œè¯·ç¨å€™...")
                
                request = JobQueryRequest(
                    city=selected_city if 'ä¸é™' not in selected_city else '',
                    areaBusiness=selected_region if 'ä¸é™' != selected_region else '',
                    position=selected_jobtype if 'ä¸é™' not in selected_jobtype else [],
                    industry=selected_industry if 'ä¸é™' != selected_industry else [],
                    degree=selected_degree if 'ä¸é™' not in selected_degree else [],
                    experience=selected_experience if 'ä¸é™' not in selected_experience else [],
                    scale=selected_scale if 'ä¸é™' not in selected_scale else [],
                    stage=selected_stage if 'ä¸é™' not in selected_stage else [],
                    keyword=keyword
                )
                url = request.to_url()
                job_buffer = multiprocessing.Queue()
                writer_process = multiprocessing.Process(target=crawling_thread, args=(job_buffer, request.jobType, url, output_file))
                writer_process.start()
                
                while True:
                    try:
                        item = job_buffer.get()
                        if item is None:
                            break
                        if isinstance(item, tuple):
                            bar.progress(item[0]/item[1], f"å·²å®Œæˆ{item[0]}é¡µï¼Œå…±{item[1]}é¡µ")
                        else:
                            TotalJobInfo.append(item)
                    except KeyboardInterrupt:
                        writer_process.terminate()
                        break
                    except Exception as e:
                        logger.error(e)
                        pass
    else:
        with open(f'cache/dataset/{selected_file_or_new}/raw_data.jsonl', 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                jobinfo = JobInfo.from_db(city=data['city'], company=data['company'], jobname=data['jobname'])
                TotalJobInfo.append(jobinfo)
    
    

    render(TotalJobInfo)
        
    
if __name__ == '__main__':
    main()